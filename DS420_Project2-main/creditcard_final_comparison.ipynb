{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Results From Other Works:\n",
    "\n",
    "* 99.950% (using DNN) - https://www.kaggle.com/code/markkostantine/deep-learning-for-credit-card-fraud-detection\n",
    "* 99.970% (using XGB Classifier) - https://www.kaggle.com/code/sachinbatra/credit-card-fraud-detection-evaluation-roc-auc/notebook\n",
    "* 99.979% (using XGB Classifier) - https://www.kaggle.com/code/abdmental01/unmasking-deception-innovations-in-credit-card\n",
    "* 100% (using RandomForestClassifier) - https://www.kaggle.com/code/jumadouglas/creditcard-23\n",
    "\n",
    "<br>\n",
    "\n",
    "<p> As determining the human accuracy of detecting credit card fraud is rather complicated, we will set our goal according to the performance of other models trained on data from this dataset. As can be seen in the above notebooks, very high accuracy can usually be achieved, with one example even reaching perfect accuracy. Therefore, our goal was to get our model to as close to 100% accuracy as possible. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis/Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of the algorithms and methods from Project1, letâ€™s look at both the traditional machine learning algorithms (Random Forest, K-Nearest Neighbors, and Decision Tree) and the more advanced techniques (Ensemble Learning and Deep Neural Networks - DNNs).\n",
    "\n",
    "## Traditional Algorithms Comparison (Project1)\n",
    "\n",
    "1. Random Forest : \n",
    "Accuracy: 0.99984,\n",
    "Precision: 0.99968,\n",
    "Recall: 1.0,\n",
    "F1 Score: 0.99984\n",
    "2. K-Nearest Neighbors (KNN) : \n",
    "Accuracy: 0.99971,\n",
    "Precision: 1.0,\n",
    "Recall: 0.82051,\n",
    "F1 Score: 0.90141\n",
    "3. Decision Tree : \n",
    "Accuracy: 0.99833,\n",
    "Precision: 0.99751,\n",
    "Recall: 0.99916,\n",
    "F1 Score: 0.99833\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Best Accuracy: Random Forest\n",
    "Best Precision: KNN\n",
    "Best Recall: Random Forest\n",
    "Best F1 Score: Random Forest\n",
    "\n",
    "\n",
    "## Ensemble Learning Methods\n",
    "\n",
    "### Performance Metrics:\n",
    "\n",
    "1. AdaBoostClassifier:              Accuracy: 0.999859\n",
    "2. StackingClassifier:              Accuracy: 0.999807\n",
    "3. GradientBoostingClassifier:      Accuracy: 0.999807\n",
    "4. RandomForestClassifier:          Accuracy: 0.999719\n",
    "5. BaggingClassifier:               Accuracy: 0.999666\n",
    "6. VotingClassifier:                Accuracy: 0.999086\n",
    "7. HistGradientBoostingClassifier:  Accuracy: 0.992192\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Best Accuracy: AdaBoostClassifier\n",
    "All ensemble methods perform exceptionally well, with AdaBoostClassifier achieving the highest accuracy.\n",
    "\n",
    "## Deep Neural Networks (DNNs)\n",
    "\n",
    "Performance Metrics:\n",
    "Precision: 1.00\n",
    "Recall: 1.00\n",
    "F1 Score: 1.00\n",
    "Accuracy: 1.00\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Perfect Scores: DNNs achieve perfect precision, recall, F1 score, and accuracy, indicating excellent performance on this dataset.\n",
    "\n",
    "### Comparative Analysis\n",
    "\n",
    "#### Accuracy:\n",
    "\n",
    "1. DNNs: Achieve perfect accuracy (1.00), outperforming all other methods.\n",
    "2. Ensemble Learning: AdaBoostClassifier comes very close with an accuracy of 0.999859.\n",
    "3. Traditional Algorithms: Random Forest performs the best among traditional methods with an accuracy of 0.99984.\n",
    "#### Precision, Recall, and F1 Score:\n",
    "\n",
    "1. DNNs: Again, DNNs achieve perfect scores in precision, recall, and F1, indicating they are exceptionally well-suited for this dataset.\n",
    "2. Traditional Algorithms: KNN achieves the highest precision, but its recall is much lower compared to Random Forest and Decision Tree. Random Forest achieves the highest recall and F1 score among traditional algorithms.\n",
    "\n",
    "### Conclusions :\n",
    "\n",
    "1. Best Overall Performance: Deep Neural Networks (DNNs) show the best overall performance with perfect scores across all metrics. This suggests they are highly effective for the given dataset.\n",
    "\n",
    "2. Best Traditional Algorithm: Among traditional algorithms, Random Forest consistently performs the best across all metrics, making it the best choice among the traditional methods evaluated.\n",
    "\n",
    "3. Ensemble Learning: While AdaBoostClassifier shows the best accuracy among ensemble methods, the slight variations in accuracy among different ensemble techniques suggest that all evaluated ensemble methods are robust and effective for this dataset.\n",
    "\n",
    "4. We can see that we are able to achieve our predicted accuracy score with almost all the algorithms and DNNs used. Also a lot of other people research on this dataset has lead to similar scores hence the results from the alogrithms and DNN seems correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
